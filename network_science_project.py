# -*- coding: utf-8 -*-
"""Network_Science_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XD6Uau8hZM0tFftVIVhig8YsbR7ROw59
"""

!pip install python-igraph==0.9.6
!pip install cairocffi
!pip install scikit-network

import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pylab as plt
import igraph as ig
import itertools
import ast
import cairocffi as cairo
import random
import math
from networkx.algorithms import community
from sknetwork.clustering import Louvain, get_modularity
from networkx.algorithms.shortest_paths.weighted import single_source_dijkstra
from sknetwork.clustering import PropagationClustering
from networkx.readwrite import json_graph
import json

from google.colab import drive
drive.mount('/content/drive')

"""
Function that we'll use later for add the value in barplots
"""
def addlabels(x,y):
    for i in range(len(x)):
        plt.text(i, y[i], y[i], ha = 'center')

"""# **Harry Potter**"""

relations = pd.read_csv('relations.csv', sep=',')
characters  = pd.read_csv('characters (1).csv', sep=";")

#import datasets
dataset_path = '/content/drive/MyDrive/Università/MagistraleUNIPD/Network Science/relations.csv'
relations = pd.read_csv(dataset_path, sep=",")
dataset_path = '/content/drive/MyDrive/Università/MagistraleUNIPD/Network Science/characters(1).csv'
characters = pd.read_csv(dataset_path, sep=";")

#delete self loops
self_loop = []
for i in range(len(relations)):
  if relations["source"][i] == relations["target"][i]:
    self_loop.append(i)

relations = relations.drop(set(self_loop)).reset_index()

#replace ids with the character names and weigths

not_numeric_relations = relations.copy()

for i in range(len(characters)):
  not_numeric_relations = not_numeric_relations.replace(i, characters["name"][i])
not_numeric_relations = not_numeric_relations.replace("-", -1)
not_numeric_relations = not_numeric_relations.replace("+", 1)

#create empty network
G=nx.Graph()

#add nodes and edges (with weight)
for i in range(len(not_numeric_relations)):
  G.add_edge(not_numeric_relations["source"][i], not_numeric_relations["target"][i], weight = not_numeric_relations["type"][i])

#add attributes
nx.set_node_attributes(G, dict(zip(characters["name"],characters["House"])), "House")
nx.set_node_attributes(G, dict(zip(characters["name"],characters["Side"])), "Side")

g = ig.Graph.from_networkx(G)

"""### First analysis

Graph
"""

#Density
visual_style = {}

nodes = g.vs()
edges = g.es()

ig.plot(g, **visual_style)

nx.draw_circular(G)

"""Metrics"""

nodes = g.vs()
edges = g.es()

print("Number of possible edges (N*(N-1)):", int(len(nodes)*(len(nodes)-1)/2))
print("Graph density:", g.density())

#list of nodes
print("List of node ids:")
G.nodes

#number of nodes
print("The total number of nodes is",len(G.nodes))

#number of edges
print("The total number of edges is",len(G.edges()))

"""Distribution of weigths and attributes"""

pos = 0
neg = 0

for i in range(len(G.edges())):
  if G[list(G.edges())[i][0]][list(G.edges())[i][1]]['weight'] == 1:  #if the weight = 1
    pos = pos+1
  else:
    neg=neg+1

print("Positive edges:", pos, "negative edges:", neg)

plt.bar(["Positive", "Negative"],[pos,neg])
addlabels(["Positive", "Negative"],[pos,neg])
plt.legend()
plt.xlabel("Weigth")
plt.ylabel("Frequency")
plt.title("Frequency of the positive/negative weigths")
plt.show()

#house
Gryffindor = 0
Ravenclaw = 0
Hufflepuff = 0
Slytherin = 0
Nonen = 0
Unkown = 0

#side
Bad=0
Good=0
Neutral=0

for node in G.nodes():
  if G.nodes[node]['House'] == "Gryffindor":
    Gryffindor=Gryffindor+1
  elif G.nodes[node]['House'] == "Ravenclaw":
    Ravenclaw=Ravenclaw+1
  elif G.nodes[node]['House'] == "Hufflepuff":
    Hufflepuff=Hufflepuff+1
  elif G.nodes[node]['House'] == "Slytherin":
    Slytherin=Slytherin+1
  elif G.nodes[node]['House'] == "None":
    Nonen=Nonen+1
  elif G.nodes[node]['House'] == "Unkown":
    Unkown=Unkown+1

  if G.nodes[node]['Side'] == "Bad":
    Bad=Bad+1
  elif G.nodes[node]['Side'] == "Good":
    Good=Good+1
  else:
    Neutral=Neutral+1

print("Gryffindor:", Gryffindor, "Ravenclaw:", Ravenclaw, "Hufflepuff:", Hufflepuff, "Slytherin:", Slytherin, "None:", Nonen, "Unkown:", Unkown)
print("Bad:", Bad, "Good:", Good, "Neutral:", Neutral)

plt.bar(["Gryffindor", "Ravenclaw", "Hufflepuff", "Slytherin", "None", "Unkown"],[Gryffindor,Ravenclaw,Hufflepuff,Slytherin,Nonen,Unkown])
addlabels(["Gryffindor", "Ravenclaw", "Hufflepuff", "Slytherin", "None", "Unkown"],[Gryffindor,Ravenclaw,Hufflepuff,Slytherin,Nonen,Unkown])
plt.legend()
plt.xlabel("House")
plt.ylabel("Frequency")
plt.title("Frequency of the characters' houses")
plt.show()

plt.bar(["Bad", "Good", "Neutral"],[Bad,Good,Neutral])
addlabels(["Bad", "Good", "Neutral"],[Bad,Good,Neutral])
plt.legend()
plt.xlabel("Side")
plt.ylabel("Frequency")
plt.title("Frequency of the characters' sides")
plt.show()

"""Distribution"""

#Generate an appropriate ensemble of null-model networks, such as Erdős–Rényi random graphs, or Maslov–Sneppen random graphs.
random_network = nx.erdos_renyi_graph(len(G.nodes()), len(G.edges())/(len(G.nodes())*(len(G.nodes()) - 1)/2), seed=None, directed=False)

#print random netwrok
nx.draw(random_network)

#number of original edges = 330
print("number of nodes:",len(random_network.nodes()), "number of edges", len(random_network.edges()))

#Calculate the average of the mean shortest path length Lr over this ensemble of null-model networks; calculate Cr analogously.
print("average shortest path for the random network", nx.average_shortest_path_length(random_network))
print("clustering coefficient for the random network", nx.average_clustering(random_network))

#Calculate the normalised shortest path λ:=L/Lr. and γ:=C/Cr
print("normalised average shortest path for the random network", nx.average_shortest_path_length(G)/nx.average_shortest_path_length(random_network))
print("normalised clustering coefficient for the random network", nx.average_clustering(G)/nx.average_clustering(random_network))

#If λ and γ fulfil certain criteria (e.g., λ≈1 and γ>1), call the network a small-world network.

"""
The idea behind this is that:
- Small-world networks should have some spatial structure, which is reflected by a high clustering coefficient.
By contrast, random networks have no such structure and a low clustering coefficient.
- Small-world networks are efficient in communicating and similar and thus have a small shortest path length,
comparable to that of random networks. By contrast, purely spatial networks have a high shortest path length.
"""

if int(nx.average_shortest_path_length(G)/nx.average_shortest_path_length(random_network)) == 1 and nx.average_clustering(G)/nx.average_clustering(random_network)>1:
  print("This network is a small network!!")
else:
  print("This network is NOT a small network :( )")

#another  proof:
sigma = nx.sigma(G, niter=5, nrand=10, seed=None)
if sigma > 1:
  print("confermed! The value of sigma is", sigma)

#another  proof:
omega = nx.omega(G, niter=5, nrand=10, seed=None)

if omega < 0.3 and omega > -0.3:
  print("confermed! The value of omega is",  omega)

"""Metrics"""

print("Is the graph connected:","yes" if g.is_connected() else "no")

#Presence of isolated components
if not g.is_connected():
  for component in g.components():
    print(component)

"""Diameter - the highest distance in the network"""

diameter = nx.diameter(G)
diameter

#Get nodes in the diameter path
diameter_path = []
d = g.get_diameter()
for i in range(0, g.diameter()):
  diameter_path.append((d[i], d[i+1]))

#Get edges in the diameter path
diameter_edges = g.get_eids(pairs=diameter_path, directed=False)

print("Path between most distant nodes: \n", [nodes[index]["_nx_name"] for index in diameter_path])

#Color the diameter path
visual_style["vertex_color"] = ["red" if node.index in list(sum(diameter_path, ())) else "grey" for node in nodes]
visual_style["edge_color"] = ["red" if edge.index in diameter_edges else "grey" for edge in edges]

ig.plot(g, **visual_style)

"""Shortest paths"""

sp = nx.shortest_path(G) #between all nodes
print(sp)

i = 0
for _, v in sp.items():
  for char, dic in v.items():
    if len(dic) == 4:
      i = i+1

print("The number of the shortest path long as the diameter are", i)

#Average shortest paths
avg_sp = nx.average_shortest_path_length(G)
print(avg_sp)

"""Degree"""

#Compute the average degree of each node’s neighbours
def sum_neigh_degree(g):
  data = {}
  for n in g.nodes():
    if g.degree(n):
      data[n] = float(sum(g.degree(i) for i in g[n]))
      g.degree(n)
  return data

sum_neigh_degree = sum_neigh_degree(G)

degree = []
for n in G.nodes:
  degree.append(G.degree(n))
  #print(f'node: {g} degree: {G.degree(g)}')

degree_df = pd.DataFrame(data = list(zip(G.nodes(), degree, sum_neigh_degree.values())), columns = ['Node', 'Degree', "Average degree of node's neighbours"])
degree_df = degree_df.sort_values(by=['Degree'], ascending=False)

print("First 5 nodes with highest degree")
print(degree_df.head(5))
print("\n")
print("Last 5 nodes with lowest degree")
print(degree_df.tail(5))

dmax = max(degree_df['Degree'])
print(f'maximum degree: {dmax}')
dmin = min(degree_df['Degree'])
print(f'minimum degree: {dmin}')
davg = np.mean(degree_df['Degree'])
print(f'average degree: {davg}')
dmostfreq = list(dict(degree_df['Degree'].value_counts()).items())[0]
print(f'most frequent degree (mode): {dmostfreq[0]} with {dmostfreq[1]} nodes')

fig = plt.figure("Degree of a random graph", figsize=(8, 8))

axgrid = fig.add_gridspec(5, 4)

ax0 = fig.add_subplot(axgrid[3:, :2])
ax0.plot(sorted(degree_df['Degree'],reverse=True), "b-", marker="o")
ax0.set_title("Degree Rank Plot")
ax0.set_ylabel("Degree")
ax0.set_xlabel("Rank")

ax1 = fig.add_subplot(axgrid[3:, 2:])
ax1.bar(*np.unique(degree_df['Degree'], return_counts=True))
ax1.set_title("Degree histogram")
ax1.set_xlabel("Degree")
ax1.set_ylabel("# of Nodes")

fig.tight_layout()
plt.show()

visual_style["vertex_size"] = [degree if degree>10 else 8 for degree in g.degree()]
visual_style['edge_color'] = 'black'
visual_style['vertex_color'] = 'red'
ig.plot(g, **visual_style)

"""Bridges"""

if nx.has_bridges(G):
  print("This netwrok has bridges.")
  bridges = list(nx.bridges(G))
  print("The bridges are:",bridges)
else:
  print("This netwrok hasn't bridges.")

visual_style = {}

bridges_index = g.bridges()

visual_style["vertex_color"] = 'grey'
visual_style["edge_color"] = ["red" if edge.index in bridges_index else "grey" for edge in edges]

ig.plot(g, **visual_style)

"""Giant components"""

Gcc = sorted(nx.connected_components(G), key=len, reverse=True)
G0 = G.subgraph(Gcc[0])

if nx.is_isomorphic(G, G0):
  print("This network is connected and there are not isolated nodes")
else:
  print("This netwrok has",len(G0),"components")

"""Centrality"""

# Closeness centrality
clo_cen = nx.closeness_centrality(G)
# Betweenness centrality
bet_cen = nx.betweenness_centrality(G)
# Eigenvector centrality
eig_cen = nx.eigenvector_centrality(G)
# Harmonic centrality
har_cen = nx.harmonic_centrality(G)
# degree centrality
degree_cen = nx.degree_centrality(G)

centrality_pd = pd.DataFrame(data = list(zip(bet_cen.keys(), bet_cen.values(), clo_cen.values(), eig_cen.values(), [1/a for a in list(har_cen.values())], degree_cen.values())),
                             columns = ['Node', 'Betweenness centrality', 'Closeness centrality', 'Eigenvector centrality', 'Harmonic centrality', 'Degree centrality'])
centrality_pd

#means of the centralities
print("mean values of the centralities:")
print("Betweenness:", np.mean(list(bet_cen.values())))
print("Closeness:", np.mean(list(clo_cen.values())))
print("Eigenvector:", np.mean(list(eig_cen.values())))
print("Harmonic:", 1/np.mean(list(har_cen.values())))
print("Degree:", np.mean(list(degree_cen.values())))

"""Returns a tuple (node,value) with the node with largest value from Networkx centrality dictionary.
To find the most central nodes we will use Python’s list comprehension technique to do basic data manipulation on our centrality dictionaries"""

def highest_centrality(cent_dict):
  # Create ordered tuple of centrality data
  cent_items=[(b,a) for (a,b) in cent_dict.items()]
  # Sort in descending order
  cent_items.sort()
  cent_items.reverse()
  return tuple(reversed(cent_items[0]))

"""Returns a tuple (node,value) with the node with largest value from Networkx centrality dictionary.
To find the x most central nodes we will use Python’s list comprehension technique to do basic data manipulation on our centrality dictionaries"""

def x_highest_centrality(cent_dict, x):
  # Create ordered tuple of centrality data
  cent_items=[(b,a) for (a,b) in cent_dict.items()]
  # Sort in descending order
  cent_items.sort()
  cent_items.reverse()
  return tuple(cent_items[0:x])

# Closeness centrality
highest_centrality_clo_cen = highest_centrality(clo_cen)
# Betweenness centrality
highest_centrality_bet_cen = highest_centrality(bet_cen)
# Eigenvector centrality
highest_centrality_eig_cen = highest_centrality(eig_cen)
# Harmonic centrality
highest_centrality_har_cen = highest_centrality(har_cen)
# degree centrality
highest_centrality_degree_cen = highest_centrality(degree_cen)

central_char = []
central_score =  []

central_char.extend([highest_centrality_clo_cen[0], highest_centrality_bet_cen[0], highest_centrality_eig_cen[0], highest_centrality_har_cen[0], highest_centrality_degree_cen[0]])
central_score.extend([highest_centrality_clo_cen[1], highest_centrality_bet_cen[1], highest_centrality_eig_cen[1], 1/highest_centrality_har_cen[1], highest_centrality_degree_cen[1]])

centrality_pd = pd.DataFrame(data = list(zip(central_char, central_score)),
                             columns = ['Most central character', 'Score'])
centrality_pd

def createCol(cen):
  central_char = []
  central_score =  []
  for i in range(len(cen)):
    central_char.append(cen[i][1])
    central_score.append(cen[i][0])
  return list(central_char), list(central_score)

#5 most central character
highest_centrality_clo_cen = x_highest_centrality(clo_cen, 5)
clo_cen_char, clo_cen_scores = createCol(highest_centrality_clo_cen)

highest_centrality_bet_cen = x_highest_centrality(bet_cen, 5)
bet_cen_char, bet_cen_scores = createCol(highest_centrality_bet_cen)

highest_centrality_eig_cen = x_highest_centrality(eig_cen, 5)
eig_cen_char, eig_cen_scores = createCol(highest_centrality_eig_cen)

highest_centrality_har_cen = x_highest_centrality(har_cen, 5)
har_cen_char, har_cen_scores = createCol(highest_centrality_har_cen)

highest_centrality_degree_cen = x_highest_centrality(degree_cen, 5)
degree_cen_char, degree_cen_scores = createCol(highest_centrality_degree_cen)

centrality_pd = pd.DataFrame(data = list(zip(bet_cen_char, bet_cen_scores, clo_cen_char, clo_cen_scores, eig_cen_char, eig_cen_scores, har_cen_char, [1/a for a in har_cen_scores], degree_cen_char, degree_cen_scores)),
                             columns = ['Most central character - Betweenness ', 'Score - Betweenness', 'Most central character - Closeness ', 'Score - Closeness ', 'Most central character - Eigenvector ', 'Score - Eigenvector ',
                             'Most central character - Harmonic ', 'Score - Harmonic ', 'Most central character - Degree ', 'Score - Degree '])
centrality_pd

#5 most central character - just names
centrality_pd = pd.DataFrame(data = list(zip(bet_cen_char, clo_cen_char, eig_cen_char, har_cen_char, degree_cen_char)),
                             columns = ['Most central character - Betweenness ', 'Most central character - Closeness ', 'Most central character - Eigenvector ',
                             'Most central character - Harmonic ', 'Most central character - Degree '])
centrality_pd

"""Homophily"""

#compute the homophily, regarding the house attribute - glogal homophily
homophily_house = {}

for char in G.nodes():
  same_house = 0
  for n in G.neighbors(char):
    if G.nodes[char]['House'] == G.nodes[n]['House']:
      same_house += 1

  homophily_house[char] = same_house/G.degree(char)

homophily_house_sorted = sorted(homophily_house.items(), key=lambda x:x[1], reverse=True)

print("The mean of the homophily level for the house attribute is", np.mean(list(homophily_house.values())))

#compute the homophily, regarding the house attribute - glogal homophily
homophily_side = []

for char in G.nodes():
  same_side = 0
  for n in G.neighbors(char):
    if G.nodes[char]['Side'] == G.nodes[n]['Side']:
      same_side += 1

  homophily_side.append(same_side/G.degree(char))

print("The mean of the homophily level for the side attribute is", np.mean(homophily_side))

#homophily for every house attrbute - local homophily
homophily_slytherin = []
homophily_hufflepuff = []
homophily_ravenclaw = []
homophily_gryffindor = []

for char in G.nodes():
  house = 0
  for n in G.neighbors(char):
    if G.nodes[char]['House'] == G.nodes[n]['House']:
      house += 1
  if G.nodes[char]['House'] == 'Ravenclaw':
    homophily_ravenclaw.append(house/G.degree(char))
  if G.nodes[char]['House'] == 'Slytherin':
    homophily_slytherin.append(house/G.degree(char))
  if G.nodes[char]['House'] == 'Gryffindor':
    homophily_gryffindor.append(house/G.degree(char))
  if G.nodes[char]['House'] == 'Hufflepuff':
    homophily_hufflepuff.append(house/G.degree(char))

print("Local homophily for every house:")
print("Gryffindor:",np.mean(homophily_gryffindor),"Slytherin",np.mean(homophily_slytherin),"Ravenclaw",np.mean(homophily_ravenclaw), "Hufflepuff", np.mean(homophily_hufflepuff) )

#homophily for every side attrbute - local homophily
homophily_good = []
homophily_bad = []
homophily_neutral = []

for char in G.nodes():
  side = 0
  for n in G.neighbors(char):
    if G.nodes[char]['Side'] == G.nodes[n]['Side']:
      side += 1
  if G.nodes[char]['Side'] == 'Good':
    homophily_good.append(side/G.degree(char))
  if G.nodes[char]['Side'] == 'Bad':
    homophily_bad.append(side/G.degree(char))
  if G.nodes[char]['Side'] == 'Neutral':
    homophily_neutral.append(side/G.degree(char))

print("Local homophily for every side:")
print("Good:",np.mean(homophily_good),"Bad",np.mean(homophily_bad),"Neutral:",np.mean(homophily_neutral))

print("assortativity coefficient for the house attribute:",nx.attribute_assortativity_coefficient(G, "House"))

print("assortativity coefficient for the side attribute:",nx.attribute_assortativity_coefficient(G, "Side"))

#Compute degree assortativity of graph.
#Assortativity measures the similarity of connections in the graph with respect to the node degree.
#It uses the potentially faster scipy.stats.pearsonr function.

print("degree assortativity coefficient:",nx.degree_pearson_correlation_coefficient(G))

hom_graph = G.copy()

to_remove = []
for char in hom_graph.nodes():
    if G.nodes[char]['Side'] == "Neutral" or G.nodes[char]['House'] == "Ravenclaw" or G.nodes[char]['House'] == "Hufflepuff":
        to_remove.append(node)

hom_graph.remove_nodes_from(to_remove)

#jaccard similarity between all couples of nodes
preds = nx.jaccard_coefficient(G, G.edges())

first_node = []
sec_node = []
score = []

for u, v, p in preds:
  first_node.append(u)
  sec_node.append(v)
  score.append(p)

jaccard_coefficient = pd.DataFrame(data = list(zip(first_node, sec_node, score)),
                             columns = ['First node', 'Second node', 'Score'])

jaccard_coefficient = jaccard_coefficient.sort_values("Score", ascending=False)
jaccard_coefficient

"""Triangles"""

#list of all traingles
triangles_list = [(n,nbr,nbr2) for n in G for nbr, nbr2 in itertools.combinations(G[n],2) if nbr in G[nbr2]]
print("List of all the edges involved in the traingles")
triangles_list

#number of triangles per character, sorted by value in descendent order

triangles_frequency = pd.DataFrame(data = list(zip(nx.triangles(G).keys(), nx.triangles(G).values())),
                             columns = ['Character', '# of triangles, which they are involved'])

triangles_frequency = triangles_frequency.sort_values("# of triangles, which they are involved", ascending=False)
triangles_frequency

#total number of triangles
print("There are", int(np.sum(list(nx.triangles(G).values())) / 3), "triangles")

#analysis of the relationships
relationships_analisys = {}   #empty dict

#initialization of the dict
relationships_analisys["+++"] = 0
relationships_analisys["++-"] = 0
relationships_analisys["+--"] = 0
relationships_analisys["---"] = 0

for i in range(len(triangles_list)):
  triangle = triangles_list[i]    #triangle
  weigth = []                     #empy list for the signs
  #save of the three signs
  weigth.append(G[triangles_list[i][0]][triangles_list[i][1]]['weight'])
  weigth.append(G[triangles_list[i][2]][triangles_list[i][1]]['weight'])
  weigth.append(G[triangles_list[i][2]][triangles_list[i][0]]['weight'])

  if weigth.count(1) == 3:                                 #counts the number of 1s
    relationships_analisys["+++"] = relationships_analisys["+++"] + 1
  elif weigth.count(1) == 2:
    relationships_analisys["++-"] = relationships_analisys["++-"] + 1
  elif weigth.count(1) == 1:
    relationships_analisys["+--"] = relationships_analisys["+--"] + 1
  elif weigth.count(1) == 0:
    relationships_analisys["---"] = relationships_analisys["---"] + 1

for el in relationships_analisys:
  relationships_analisys[el] = int(relationships_analisys[el]/3)

relationships_analisys

"""clustering coefficient"""

#clustering coefficient
#Compute the average clustering coefficient for the graph G.
print("The clustering coefficient is", nx.average_clustering(G))

triadic_closure = nx.transitivity(G)
print("Triadic closure:", triadic_closure)

#Global CC - number of closed triplets/number of possible triplets
#Transitivity measures the probability that the adjacent vertices of a vertex are connected.
#This is sometimes also called the clustering coefficient.
# --> Triadic closure
print("Global clustering coefficient", g.transitivity_undirected())

#Local CC - are the neighbours of the nodes also connected?
print("Local clustering components:")
local_ccs = g.transitivity_local_undirected()
sum_cc = 0
for local_cc in local_ccs:
  if not math.isnan(local_cc):
    sum_cc += local_cc

nodes = g.vs()
i = 0
cc = {}
clustering_coefficient = {}
for node in nodes:
  clustering_coefficient[node] = local_ccs[node.index]
  cc[list(G.nodes)[i]] = local_ccs[node.index]
  i = i+1

cc = dict(sorted(cc.items(), key=lambda x:x[1], reverse=True))

for i in range(len(cc)):
  print("Local clustering coefficient of node", list(cc.keys())[i],":", list(cc.values())[i])

"""Most loved/hated character"""

pos_count = {}      #count for every characater the number of positive edges
neg_count = {}      #count for every characater the number of negative edges

#initialization
for node in G.nodes():
  pos_count[node] = 0
  neg_count[node] = 0

#count for every characater the number of positive and negative edges
for i in range(len(G.edges())):
  if G[list(G.edges())[i][0]][list(G.edges())[i][1]]['weight'] == 1:          #if the weigth is positive
    pos_count[list(G.edges())[i][0]] = pos_count[list(G.edges())[i][0]] + 1   #first character of the couple
    pos_count[list(G.edges())[i][1]] = pos_count[list(G.edges())[i][1]] + 1   #second character of the couple
  else:
    neg_count[list(G.edges())[i][0]] = neg_count[list(G.edges())[i][0]] + 1
    neg_count[list(G.edges())[i][1]] = neg_count[list(G.edges())[i][1]] + 1

#create dataset with the results
weight_sign_per_char = pd.DataFrame(data = list(zip(G.nodes(), pos_count.values(),  neg_count.values())),
                             columns = ['Character', '# of positive edges', '# of negative edges'])

#sort the df by the number of positive edges

weight_sign_per_char = weight_sign_per_char.sort_values(by=['# of positive edges'], ascending=False)    #sort the df by the number of positive edges

most_loved = weight_sign_per_char.head(5)[["Character", "# of positive edges"]]                         #take the characters which the five highest value of positive weigths

total_degree = []     #new column for the total degree of the character
perc_degree = []      #new column for the percentage of positive degree of the character

#populated the list
for node in most_loved["Character"]:
  total_degree.append(G.degree(node))
  perc_degree.append(int(most_loved[most_loved["Character"] == node]["# of positive edges"]/G.degree(node)*100))

#add the new columns
most_loved["Total degree"] = total_degree
most_loved["Percentage degree"] = perc_degree

print("Dataset of the five most loved characters")
most_loved

#sort the df by the number of positive edges
weight_sign_per_char = weight_sign_per_char.sort_values(by=['# of negative edges'], ascending=False)    #sort the df by the number of negative edges

most_hated = weight_sign_per_char.head(5)[["Character", "# of negative edges"]]                         #take the characters which the five highest value of negative weigths

total_degree = []     #new column for the total degree of the character
perc_degree = []      #new column for the percentage of negative degree of the character

#populated the list
for node in most_hated["Character"]:
  total_degree.append(G.degree(node))
  perc_degree.append(int(most_hated[most_hated["Character"] == node]["# of negative edges"]/G.degree(node)*100))

#add the new columns
most_hated["Total degree"] = total_degree
most_hated["Percentage degree"] = perc_degree

print("Dataset of the five most hated characters")
most_hated

"""Most loved/hated character, proportionally to the number of edges"""

#normalize the counting for the number of total edges of that character
for node in G.nodes():
  if pos_count[node] + neg_count[node] >= dmostfreq[0]:
    pos_count[node] = round(pos_count[node] / G.degree(node), 3)
    neg_count[node] = round(neg_count[node] / G.degree(node), 3)
  else:
    pos_count[node] = 0
    neg_count[node] = 0

#create dataset with the results
normalized_weight_sign_per_char = pd.DataFrame(data = list(zip(G.nodes(), pos_count.values(),  neg_count.values())),
                             columns = ['Character', '# of positive edges', '# of negative edges'])

normalized_weight_sign_per_char

#sort the df by the number of positive edges

normalized_weight_sign_per_char = normalized_weight_sign_per_char.sort_values(by=['# of positive edges'], ascending=False)    #sort the df by the number of positive edges

most_loved = normalized_weight_sign_per_char.head(5)[["Character", "# of positive edges"]]                         #take the characters which the five highest value of positive weigths

total_degree = []     #new column for the total degree of the character
pos_degree = []       #new column for the correct number of positive weigth, in absolute value
perc_degree = []      #new column for the percentage of positive degree of the character

#populated the list
for node in most_loved["Character"]:
  pos_degree.append(int(most_loved[most_loved["Character"] == node]["# of positive edges"] * G.degree(node)))
  total_degree.append(G.degree(node))
  perc_degree.append(int(most_loved[most_loved["Character"] == node]["# of positive edges"]*100))

#delete the "# of positive edges" column
del most_loved["# of positive edges"]

#add the new columns
most_loved["# of positive edges"] = pos_degree
most_loved["Total degree"] = total_degree
most_loved["Percentage degree"] = perc_degree

print("Dataset of the five most loved characters")
most_loved.sort_values(by=["Percentage degree", '# of positive edges'], ascending=False)    #sort the df by the number of positive edges

#sort the df by the number of negative edges

normalized_weight_sign_per_char = normalized_weight_sign_per_char.sort_values(by=['# of negative edges'], ascending=False)    #sort the df by the number of negative edges

most_hated = normalized_weight_sign_per_char.head(5)[["Character", "# of negative edges"]]                         #take the characters which the five highest value of negative weigths

total_degree = []     #new column for the total degree of the character
pos_degree = []       #new column for the correct number of negative weigth, in absolute value
perc_degree = []      #new column for the percentage of negative degree of the character

#populated the list
for node in most_hated["Character"]:
  pos_degree.append(int(most_hated[most_hated["Character"] == node]["# of negative edges"] * G.degree(node)))
  total_degree.append(G.degree(node))
  perc_degree.append(int(most_hated[most_hated["Character"] == node]["# of negative edges"]*100))

#delete the "# of negative edges" column
del most_hated["# of negative edges"]

#add the new columns
most_hated["# of negative edges"] = pos_degree
most_hated["Total degree"] = total_degree
most_hated["Percentage degree"] = perc_degree

print("Dataset of the five most loved characters")
most_hated.sort_values(by=["Percentage degree", '# of negative edges'], ascending=False)    #sort the df by the number of negative edges

"""Community"""

#from the numveric label of the communities to the name of the characters - 2 communities detected
def convertInLabelArray_2Communities(list):
  d = []
  for node in G.nodes():
    if node in list[1]:
      d.append(1)
    elif node in list[0]:
      d.append(0)
    else:
      d.append(2)

  d = np.array(d)
  return d

#from the numveric label of the communities to the name of the characters - 3 communities detected
def convertInLabelArray_3Communities(list):
  d = []
  for node in G.nodes():
    if node in list[1]:
      d.append(1)
    elif node in list[0]:
      d.append(0)
    elif node in list[2]:
      d.append(2)
    else:
      d.append(3)

  d = np.array(d)
  return d

#from the numveric label of the communities to the name of the characters - 4 communities detected
def convertInLabelArray_4Communities(list):
  d = []
  for node in G.nodes():
    if node in list[1]:
      d.append(1)
    elif node in list[0]:
      d.append(0)
    elif node in list[2]:
      d.append(2)
    elif node in list[3]:
      d.append(3)
    else:
      d.append(4)

  d = np.array(d)
  return d

#lists of the number of communities and modularities for the graphs
communities_methods = []
n_communities = []
modularities = []

"""Infomap"""

#Python Igraph community cluster colors
i = g.community_infomap()
pal = ig.drawing.colors.ClusterColoringPalette(len(i))
g.vs['color'] = pal.get_many(i.membership)
ig.plot(g)

#putting the names of the characters in two different lists
uno = []
due = []
ll = i.membership

for i, node in enumerate(G.nodes()):
  if ll[i] == 0:
    uno.append(node)
  elif ll[i] == 1:
    due.append(node)

#print the characters, divided per communities
print("List of the caracters, divided per communities:")
print(uno)
print(due)

#modularity
modularity = np.round(g.modularity(ll), 3)
print("The modularity for the infomap method is",modularity)

communities_methods.append("Infomap")
n_communities.append(len(set(ll)))
modularities.append(modularity)

"""girvan_newman"""

#compute girvan_newman communities
com = community.girvan_newman(G)

#list of characters per communities
print("List of the caracters, divided per communities:")
tuple(sorted(c) for c in next(com))

posi_gn = nx.spring_layout(G)

k = len(next(com))   # number of communities
for _ in range(k-1):
    comms = next(com)

colors = ["red", "blue", "Chartreuse"]
for nodes, c in zip(comms, colors):
    nx.draw_networkx_nodes(G, posi_gn, nodelist=nodes, node_color=[c])
nx.draw_networkx_edges(G, posi_gn)

#modularity
modularity = np.round(get_modularity(nx.to_numpy_array(G, weight=None), convertInLabelArray_3Communities(next(com))), 3)
print("The modularity for the girvan_newman method is", modularity)

communities_methods.append("girvan_newman")
n_communities.append(k)
modularities.append(modularity)

"""k_clique_communities"""

#clique percolation (overlap)
clique = list(nx.community.k_clique_communities(G, 15))  #the second number is the smallest number for the communities

#number of communities
len(clique)

#clique percolation (overlap)- change of minimm chars
clique = list(nx.community.k_clique_communities(G, 10))  #the second number is the smallest number for the communities

#number of communities
len(clique)

#number of characters in the first community - not every character is classified
len(clique[0])

#list of characters per communities
clique

#clique percolation (overlap)- change of minimm chars
clique6 = list(nx.community.k_clique_communities(G, 6))

#number of communities
print("Clique detected",len(clique6),"communities")

#list of characters per communities
print("List of the characters divided per communities")
clique6

#number of characters in the first community - not every character is classified
print("The first communities has",len(clique6[0]),"characters")

color_map = []
for node in G:
    if node in clique6[1]:
      color_map.append('Chartreuse')
    elif node not in clique6[0]:
      color_map.append('grey')      #grey: not classified
    else:
      color_map.append('red')
nx.draw(G, node_color=color_map)
plt.show()

#modularity
modularity = np.round(get_modularity(nx.to_numpy_array(G, weight=None), convertInLabelArray_2Communities(clique6)), 3)
print("The modularity for the clique percolation method is", modularity)

communities_methods.append("clique percolation")
n_communities.append(len(clique6))
modularities.append(modularity)

"""Louvain"""

#manual addind because computed in gephi

communities_methods.append("Louvain")
n_communities.append(5)
modularities.append(0.342)

"""PropagationClustering"""

colors = ["red", "Chartreuse"]
pos = nx.spring_layout(G)
lst_m = community.label_propagation_communities(G)
color_map_b = {}
keys = G.nodes()
values = "black"
for i in keys:
        color_map_b[i] = values
counter = 0
for c in lst_m:
  for n in c:
    color_map_b[n] = colors[counter]
  counter = counter + 1
nx.draw_networkx_edges(G, pos)
nx.draw_networkx_nodes(G, pos, node_color=dict(color_map_b).values())
plt.axis("off")
plt.show()

#list of characters per communities
print("List of characters divided per communites:")
lst_m

#modularity
modularity = np.round(get_modularity(nx.to_numpy_array(G, weight=None), convertInLabelArray_2Communities(list(lst_m))), 3)
print("The modularity for the propagation clustering method is",modularity)

communities_methods.append("propagation clustering")
n_communities.append(len(lst_m))
modularities.append(modularity)

"""greedy_modularity_communities"""

#computer greedy_modularity_communities to get communities
communities = community.greedy_modularity_communities(G)

#numbers of communities
print("The number of communities detected are:",len(communities))

#create a list pf characters per every communties
uno=[]
due =[]
tre =[]
qua=[]

for node in G.nodes():
  if node in communities[0]:
    uno.append(node)
  if node in communities[1]:
    due.append(node)
  if node in communities[2]:
    tre.append(node)
  if node in communities[3]:
    qua.append(node)

#print the characters, divided per communities
print("List of characters divided per communities:")
print(uno)
print(due)
print(tre)
print(qua)

#Community visualization
colors = ["red", "blue", "Chartreuse", "magenta"]
pos = nx.spring_layout(G)
color_map_b = {}
keys = G.nodes()
values = "black"
for i in keys:
        color_map_b[i] = values
counter = 0
for x in communities:
  for n in x:
    color_map_b[n] = colors[counter]
  counter = counter + 1

nx.draw_networkx_edges(G, pos)
nx.draw_networkx_nodes(G, pos, node_color=dict(color_map_b).values())
plt.axis("off")
plt.show()

#modularity
modularity = np.round(get_modularity(nx.to_numpy_array(G, weight=None), convertInLabelArray_4Communities(communities)), 3)
print("The modularity for the greedy modularity communities is", modularity)

communities_methods.append("greedy modularity communities")
n_communities.append(len(communities))
modularities.append(modularity)

"""walktrap"""

g = ig.Graph.TupleList(G.edges(), directed=False)

#compute walktrap for getting the communities
wtrap = g.community_walktrap(steps = 4)
clust = wtrap.as_clustering()
ig.plot(clust)

#create a list pf characters per every communties
uno=[]
due =[]
tre =[]
qua=[]
cin=[]
sei=[]
sett=[]
otto = []
nove = []
dieci = []
und = []
dodici = []
tredici = []

for i, node in enumerate(G.nodes()):
  if clust.membership[i] == 0:
    uno.append(node)
  if clust.membership[i] == 1:
    due.append(node)
  if clust.membership[i] == 2:
    tre.append(node)
  if clust.membership[i] == 3:
    qua.append(node)
  if clust.membership[i] == 4:
    cin.append(node)
  if clust.membership[i] == 5:
    sei.append(node)
  if clust.membership[i] == 6:
    sett.append(node)
  if clust.membership[i] == 7:
    otto.append(node)
  if clust.membership[i] == 8:
    nove.append(node)
  if clust.membership[i] == 9:
    dieci.append(node)
  if clust.membership[i] == 10:
    und.append(node)
  if clust.membership[i] == 11:
    dodici.append(node)
  if clust.membership[i] == 12:
    tredici.append(node)

#print the characters, divided per communities
print("List of character divided per communities")
print(uno)
print(due)
print(tre)
print(qua)
print(cin)
print(sei)
print(sett)
print(otto)
print(nove)
print(dieci)
print(und)
print(dodici)
print(tredici)

#modularity
modularity = np.round(g.modularity(clust), 3)
print("The modularity for the walktrap methos is",modularity)

communities_methods.append("walktrap")
n_communities.append(len(set(clust.membership)))
modularities.append(modularity)

"""Plot"""

#Number of communites per method
plt.bar(communities_methods,n_communities)
addlabels(communities_methods,n_communities)
plt.legend()
plt.xticks(rotation='vertical')
plt.xlabel("Method")
plt.ylabel("# communities")
plt.title("Number of communites per method")
plt.show()

#Modularities per method
plt.bar(communities_methods,modularities)
addlabels(communities_methods,modularities)
plt.legend()
plt.xticks(rotation='vertical')
plt.xlabel("Method")
plt.ylabel("Modularity")
plt.title("Modularities per method")
plt.show()

"""Robusteness

Random nodes removal
"""

random_seed = 42
random.seed(random_seed)

def random_removal(graph, nodes):
    graph_copy = graph.copy()
    nodes_to_remove = random.sample(graph_copy.nodes(), nodes)
    graph_copy.remove_nodes_from(nodes_to_remove)
    return graph_copy

random_res = {}
graphs = []
n_sample_to_remove = [5, 10, 15, 20, 25, 30, 35]

for i in n_sample_to_remove:
    new_graph = random_removal(G, i)
    a = nx.connected_components(new_graph)
    components = [len(c) for c in sorted(a, key=len, reverse=True)]
    #compute all the new metrics
    #five most central nodes
    first5_bcen = list(dict(sorted(nx.betweenness_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_ccen = list(dict(sorted(nx.closeness_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_ecen = list(dict(sorted(nx.eigenvector_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_charm = list(dict(sorted(nx.harmonic_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_cdegree = list(dict(sorted(nx.degree_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]

    #mean centrality
    mean_bcen = np.mean(list(nx.betweenness_centrality(new_graph).values()))
    mean_ccen = np.mean(list(nx.closeness_centrality(new_graph).values()))
    mean_ecen = np.mean(list(nx.eigenvector_centrality(new_graph).values()))
    mean_charm = np.mean(list(nx.harmonic_centrality(new_graph).values()))
    mean_cdegree = np.mean(list(nx.degree_centrality(new_graph).values()))

    #metric about the giant component - impossible to compute otherwise
    Gcc = sorted(nx.connected_components(new_graph), key=len, reverse=True)
    giant_component = G.subgraph(Gcc[0])

    avg_shortest_path = nx.average_shortest_path_length(giant_component)    #average shortest path
    d = nx.diameter(giant_component)                                        #diameter

    #dictionary
    random_res[i] = {'average shortest path': avg_shortest_path,
                         'diameter': d,
                         '5 most central nodes - betwennes centrality': first5_bcen,
                         '5 most central nodes - closeness centrality': first5_ccen,
                         '5 most central nodes - eigenvector centrality': first5_ecen,
                         '5 most central nodes - harmonic centrality': first5_ccen,
                         '5 most central nodes - degree centrality': first5_ecen,
                         'Mean betwennes centrality': mean_bcen,
                         'Mean closeness centrality': mean_ccen,
                         'Mean eigenvector centrality': mean_ecen,
                         'Mean harmonic centrality': mean_ccen,
                         'Mean degree centrality': mean_ecen,
                         'number of components': len(components)}
    graphs.append(new_graph)

for i in n_sample_to_remove:
  print(random_res[i])

#Get the nodes that now are without any link
for i in range(len(n_sample_to_remove)):
  Gcc = sorted(nx.connected_components(graphs[i]), key=len, reverse=True)
  if len(Gcc) > 1:
    Gcc.pop(0)
    print("Isolated nodes in the network: ",i, "is", Gcc)

"""Based on homophily"""

# Count nodes with the same attribute Slytherin
slytherin = [node for node, attributes in G.nodes(data=True) if attributes.get('House') == 'Slytherin']
# Count nodes with the same attribute Gryffindor
gryffindor = [node for node, attributes in G.nodes(data=True) if attributes.get('House') == 'Gryffindor']

def percentage(part, whole):
  return round((part*whole)/100)

slytherin_ten = percentage(10, len(slytherin))
slytherin_thirty = percentage(30, len(slytherin))
gryffindor_ten = percentage(10, len(gryffindor))
gryffindor_thirty = percentage(30, len(gryffindor))

def homophily_removal(graph, nodes, h):
    graph_copy = graph.copy()
    nodes_to_remove = []
    k = nodes
    l = 0
    while k >= 0:
      if G.nodes[homophily_house_sorted[l][0]]['House'] == h:
        nodes_to_remove.append(homophily_house_sorted[l][0])
        k = k - 1
      l = l + 1
    graph_copy.remove_nodes_from(nodes_to_remove)
    return graph_copy

homophily_house_res = {}
graphs = []
n_sample_to_remove = [slytherin_ten,slytherin_thirty,gryffindor_ten,gryffindor_thirty]
house = ["Slytherin", "Slytherin", "Gryffindor", "Gryffindor"]

j = 0
for i in n_sample_to_remove:
    new_graph = homophily_removal(G, i, house[j])
    a = nx.connected_components(new_graph)
    components = [len(c) for c in sorted(a, key=len, reverse=True)]
    j = j + 1

    #compute all the new metrics
    #five most central nodes
    first5_bcen = list(dict(sorted(nx.betweenness_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_ccen = list(dict(sorted(nx.closeness_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_ecen = list(dict(sorted(nx.eigenvector_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_charm = list(dict(sorted(nx.harmonic_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_cdegree = list(dict(sorted(nx.degree_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]

    #mean centrality
    mean_bcen = np.mean(list(nx.betweenness_centrality(new_graph).values()))
    mean_ccen = np.mean(list(nx.closeness_centrality(new_graph).values()))
    mean_ecen = np.mean(list(nx.eigenvector_centrality(new_graph).values()))
    mean_charm = np.mean(list(nx.harmonic_centrality(new_graph).values()))
    mean_cdegree = np.mean(list(nx.degree_centrality(new_graph).values()))

    #metric about the giant component - impossible to compute otherwise
    Gcc = sorted(nx.connected_components(new_graph), key=len, reverse=True)
    giant_component = G.subgraph(Gcc[0])

    avg_shortest_path = nx.average_shortest_path_length(giant_component)    #average shortest path
    d = nx.diameter(giant_component)                                        #diameter

    #dictionary
    homophily_house_res[i] = {'average shortest path': avg_shortest_path,
                         'diameter': d,
                         '5 most central nodes - betwennes centrality': first5_bcen,
                         '5 most central nodes - closeness centrality': first5_ccen,
                         '5 most central nodes - eigenvector centrality': first5_ecen,
                         '5 most central nodes - harmonic centrality': first5_ccen,
                         '5 most central nodes - degree centrality': first5_ecen,
                         'Mean betwennes centrality': mean_bcen,
                         'Mean closeness centrality': mean_ccen,
                         'Mean eigenvector centrality': mean_ecen,
                         'Mean harmonic centrality': mean_ccen,
                         'Mean degree centrality': mean_ecen,
                         'number of components': len(components)}

    graphs.append(new_graph)

for i in n_sample_to_remove:
  print(homophily_house_res[i])

#Get the nodes that now are without any link
control = 0         #to know if there are isolated nodes
for i in range(len(n_sample_to_remove)):
  Gcc = sorted(nx.connected_components(graphs[i]), key=len, reverse=True)
  if len(Gcc) > 1:
    Gcc.pop(0)
    print("Isolated nodes in the network: ",i, "is", Gcc)
    control = 1

if control == 0:
  print("There are not isolated nodes")

"""Based on centrality"""

def centrality_removal(graph, nodes):
    graph_copy = graph.copy()
    nodes_to_remove = list(centrality_pd["Most central character - Betweenness "][0:nodes]) #list of the nodes to remove
    graph_copy.remove_nodes_from(nodes_to_remove)   #remove the nodes
    return graph_copy

centrality_res = {}           #dictionary in with append the new metrics
graphs = []                   #list of the new graphs to visualize
n_sample_to_remove = [1,3,5]  #list of number of nodes to delete

for i in n_sample_to_remove:
    new_graph = centrality_removal(G, i)      #remove nodes
    a = nx.connected_components(new_graph)    #get the components
    components = [len(c) for c in sorted(a, key=len, reverse=True)] #count the number of componentes (>1 if disconnected)
    """
    if len(components) ==1:   #if the grapgh is connected (never in this graph)
      #compute all the new metrics
      d =  nx.diameter(new_graph)
      s = nx.shortest_path(new_graph)
      a_s = nx.average_shortest_path_length(new_graph)
      bcen = nx.betweenness_centrality(new_graph)
      ccen = nx.closeness_centrality(new_graph)
      ecen = nx.eigenvector_centrality(new_graph)
      centrality_res[i] = {'diameter' : d, 'shortest path': s, 'average shortest path': a_s, 'betwennes centrality': bcen, 'closeness centrality': ccen,'eigenvector centrality': ecen}
    else:   #if the grapgh isn't connected
    """
    #compute all the new metrics
    #five most central nodes
    first5_bcen = list(dict(sorted(nx.betweenness_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_ccen = list(dict(sorted(nx.closeness_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_ecen = list(dict(sorted(nx.eigenvector_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_charm = list(dict(sorted(nx.harmonic_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]
    first5_cdegree = list(dict(sorted(nx.degree_centrality(new_graph).items(), key=lambda x:x[1], reverse=True)).keys())[0:5]

    #mean centrality
    mean_bcen = np.mean(list(nx.betweenness_centrality(new_graph).values()))
    mean_ccen = np.mean(list(nx.closeness_centrality(new_graph).values()))
    mean_ecen = np.mean(list(nx.eigenvector_centrality(new_graph).values()))
    mean_charm = np.mean(list(nx.harmonic_centrality(new_graph).values()))
    mean_cdegree = np.mean(list(nx.degree_centrality(new_graph).values()))

    #metric about the giant component - impossible to compute otherwise
    Gcc = sorted(nx.connected_components(new_graph), key=len, reverse=True)
    giant_component = G.subgraph(Gcc[0])

    avg_shortest_path = nx.average_shortest_path_length(giant_component)    #average shortest path
    d = nx.diameter(giant_component)                                        #diameter

    #dictionary
    centrality_res[i] = {'average shortest path': avg_shortest_path,
                         'diameter': d,
                         '5 most central nodes - betwennes centrality': first5_bcen,
                         '5 most central nodes - closeness centrality': first5_ccen,
                         '5 most central nodes - eigenvector centrality': first5_ecen,
                         '5 most central nodes - harmonic centrality': first5_ccen,
                         '5 most central nodes - degree centrality': first5_ecen,
                         'Mean betwennes centrality': mean_bcen,
                         'Mean closeness centrality': mean_ccen,
                         'Mean eigenvector centrality': mean_ecen,
                         'Mean harmonic centrality': mean_ccen,
                         'Mean degree centrality': mean_ecen,
                         'number of components': len(components)}
    graphs.append(new_graph)

centrality_res[1]

centrality_res[3]

centrality_res[3]

#Get the nodes that now are without any link

Gcc = sorted(nx.connected_components(graphs[0]), key=len, reverse=True)
Gcc.pop(0)
print("Isolated nodes in the first network: ",Gcc)

Gcc = sorted(nx.connected_components(graphs[1]), key=len, reverse=True)
Gcc.pop(0)
print("Isolated nodes in the second network: ",Gcc)

Gcc = sorted(nx.connected_components(graphs[2]), key=len, reverse=True)
Gcc.pop(0)
print("Isolated nodes in the third network: ",Gcc)

"""Preferential attachment

Link prediction
"""

#preferential attachment
preds = nx.preferential_attachment(G, list(nx.non_edges(G)))

first_node = []
sec_node = []
score = []

for u, v, p in preds:
  first_node.append(u)
  sec_node.append(v)
  score.append(p)

preferential_attachment = pd.DataFrame(data = list(zip(first_node, sec_node, score)),
                             columns = ['First node', 'Second node', 'Score'])

preferential_attachment = preferential_attachment.sort_values("Score", ascending=False)
preferential_attachment

#adamic_adar_index
preds = nx.adamic_adar_index(G, list(nx.non_edges(G)))

first_node = []
sec_node = []
score = []

for u, v, p in preds:
  first_node.append(u)
  sec_node.append(v)
  score.append(p)

adamic_adar_index = pd.DataFrame(data = list(zip(first_node, sec_node, score)),
                             columns = ['First node', 'Second node', 'Score'])

adamic_adar_index = adamic_adar_index.sort_values("Score", ascending=False)
adamic_adar_index

#jaccard similarity
preds = nx.jaccard_coefficient(G, list(nx.non_edges(G)))

first_node = []
sec_node = []
score = []

for u, v, p in preds:
  first_node.append(u)
  sec_node.append(v)
  score.append(p)

jaccard_coefficient = pd.DataFrame(data = list(zip(first_node, sec_node, score)),
                             columns = ['First node', 'Second node', 'Score'])

jaccard_coefficient = jaccard_coefficient.sort_values("Score", ascending=False)
jaccard_coefficient

"""Node prediction

Adding one node with one edge
"""

#Returns a random graph using Barabási–Albert preferential attachment
#A graph of nodes is grown by attaching new nodes each with edges that are preferentially attached to existing nodes with high degree.

#the first number n is how many node we want to have in the new graph (old + new nodes)
#and the second number m is the umber of edges to attach from a new node to existing nodes

#Barabási–Albert network must have m >= 1 and m < n
one_node_one_edge = nx.barabasi_albert_graph(len(G.nodes())+1, 1, seed=42, initial_graph=G)

#adding a new node - preferential attachment
preds = nx.preferential_attachment(one_node_one_edge, one_node_one_edge.edges())

first_node = []
sec_node = []
score = []

for u, v, p in preds:
  first_node.append(u)
  sec_node.append(v)
  score.append(p)

preferential_attachment = pd.DataFrame(data = list(zip(first_node, sec_node, score)),
                             columns = ['First node', 'Second node', 'Score'])

preferential_attachment = preferential_attachment.sort_values("Score", ascending=False)
preferential_attachment[(preferential_attachment["First node"] == len(G)) | (preferential_attachment["Second node"] == len(G))]

"""Adding one node with ten edges"""

#Returns a random graph using Barabási–Albert preferential attachment
#A graph of nodes is grown by attaching new nodes each with edges that are preferentially attached to existing nodes with high degree.

#the first number n is how many node we want to have in the new graph (old + new nodes)
#and the second number m is the umber of edges to attach from a new node to existing nodes

#Barabási–Albert network must have m >= 1 and m < n
one_node_ten_edge = nx.barabasi_albert_graph(len(G.nodes())+1, 10, seed=42, initial_graph=G)

#adding a new node - preferential attachment
preds = nx.preferential_attachment(one_node_ten_edge, one_node_ten_edge.edges())

first_node = []
sec_node = []
score = []

for u, v, p in preds:
  first_node.append(u)
  sec_node.append(v)
  score.append(p)

preferential_attachment = pd.DataFrame(data = list(zip(first_node, sec_node, score)),
                             columns = ['First node', 'Second node', 'Score'])

preferential_attachment = preferential_attachment.sort_values("Score", ascending=False)
preferential_attachment[(preferential_attachment["First node"] == len(G)) | (preferential_attachment["Second node"] == len(G))]

"""Adding ten nodes with x edges, where x is the average number of the edges per node"""

#Returns a random graph using Barabási–Albert preferential attachment
#A graph of nodes is grown by attaching new nodes each with edges that are preferentially attached to existing nodes with high degree.

#the first number n is how many node we want to have in the new graph (old + new nodes)
#and the second number m is the umber of edges to attach from a new node to existing nodes

#Barabási–Albert network must have m >= 1 and m < n
nodes_to_add = 10
ten_node_three_edge = nx.barabasi_albert_graph(len(G.nodes())+nodes_to_add, int(davg), seed=42, initial_graph=G)

#adding a new node - preferential attachment
preds = nx.preferential_attachment(ten_node_three_edge, ten_node_three_edge.edges())

first_node = []
sec_node = []
score = []

for u, v, p in preds:
  first_node.append(u)
  sec_node.append(v)
  score.append(p)

preferential_attachment = pd.DataFrame(data = list(zip(first_node, sec_node, score)),
                             columns = ['First node', 'Second node', 'Score'])

preferential_attachment = preferential_attachment.sort_values("Score", ascending=False)
filtered_preferential_attachment = preferential_attachment[(preferential_attachment['First node'].isin(np.arange(len(G.nodes()), len(G.nodes())+nodes_to_add))) |
                                                           (preferential_attachment['Second node'].isin(np.arange(len(G.nodes()), len(G.nodes())+nodes_to_add)))]
filtered_preferential_attachment

filtered_preferential_attachment_counts = filtered_preferential_attachment['First node'].value_counts()
filtered_preferential_attachment_counts

filtered_chars =  list(dict(filtered_preferential_attachment_counts).keys())[0:10]
filtered_values = list(dict(filtered_preferential_attachment_counts).values())[0:10]

#Modularities per method
plt.bar(filtered_chars,filtered_values)
addlabels(filtered_chars,filtered_values)
plt.legend()
plt.xticks(rotation='vertical')
plt.xlabel("Characters")
plt.ylabel("Frequency in which a node were linked")
plt.title("The most influenced characters")
plt.show()